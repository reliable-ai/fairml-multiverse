{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f58c32f",
   "metadata": {},
   "source": [
    "The following cell holds the definition of our parameters,\n",
    "these values can be overriden by rendering the with e.g. the following command:\n",
    "\n",
    "```bash\n",
    "papermill -p alpha 0.2 -p ratio 0.3 universe_analysis.ipynb output/test_run.ipynb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c53745c",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "run_no = 0\n",
    "universe_id = \"test\"\n",
    "universe = {\n",
    "    \"scale\": \"scale\", # \"scale\", \"do-not-scale\",\n",
    "    \"encode_categorical\": \"one-hot\", # \"ordinal\", \"one-hot\"\n",
    "    \"stratify_split\": \"none\", # \"none\", \"target\", \"protected-attribute\", \"both\",\n",
    "    \"model\": \"elasticnet\", # \"logreg\", \"rf\", \"svm\", \"gbm\", \"elasticnet\"\n",
    "    \"cutoff\": [\"raw_0.5\", \"quantile_0.1\", \"quantile_0.25\"],\n",
    "    \"preprocess_age\": \"quantiles_3\", # \"none\", \"bins_10\", \"quantiles_3\", \"quantiles_4\"\n",
    "    \"preprocess_income\": \"bins_10000\", # \"none\", \"log\", \"bins_10000\", \"quantiles_3\", \"quantiles_4\"\n",
    "    \"exclude_features\": \"none\", # \"race\", \"sex\", \"race-sex\"\n",
    "    \"exclude_subgroups\": \"drop-name_race_Some Other Race alone\", # keep-all, drop-smallest_race_2, keep-largest_race_1, keep-largest_race_2, drop-name_race_Some Other Race alone\n",
    "    \"eval_fairness_grouping\": [\"majority-minority\", \"race-all\"],\n",
    "    \"eval_exclude_subgroups\": [\"exclude-in-eval\", \"keep-in-eval\"],\n",
    "    \"eval_on_subset\": [\n",
    "        \"full\",\n",
    "        # Largest PUMA region\n",
    "        \"locality-largest-only\",\n",
    "        # PUMA region w/ highest share of white people\n",
    "        \"locality-whitest-only\",\n",
    "        # PUMA regions belonging to a large city\n",
    "        \"locality-city-la\",\n",
    "        \"locality-city-sf\",\n",
    "        # Exclude military personnel from test dataset\n",
    "        \"exclude-military\",\n",
    "        # Exclude non US citizens from test dataset\n",
    "        \"exclude-non-citizens\",\n",
    "    ],\n",
    "}\n",
    "output_dir=\"./output\"\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115beadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Parse universe into dict if it is passed as a string\n",
    "if isinstance(universe, str):\n",
    "    universe = json.loads(universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd256654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload the custom package\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport fairness_multiverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab72b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.universe import UniverseAnalysis\n",
    "\n",
    "universe_analysis = UniverseAnalysis(\n",
    "    run_no = run_no,\n",
    "    universe_id = universe_id,\n",
    "    universe = universe,\n",
    "    output_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec02d2",
   "metadata": {},
   "source": [
    "Always use the same seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe74108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "parsed_seed = int(seed)\n",
    "np.random.seed(parsed_seed)\n",
    "print(f\"Using Seed: {parsed_seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08031d45",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Using some newer features of the `folktables` package. Therefore it needs to be installed from github directly. Also using a forked version of the package to implement a fix faster (install via `pip install --upgrade --force-reinstall git+https://github.com/jansim/folktables`).\n",
    "\n",
    "### (Down)load Data from Census\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f131bad",
   "metadata": {
    "tags": [
     "load-data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache: data/dataset.csv.gz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from folktables import ACSDataSource\n",
    "\n",
    "data_source = ACSDataSource(\n",
    "    survey_year='2018',\n",
    "    horizon='1-Year',\n",
    "    survey='person'\n",
    ")\n",
    "\n",
    "# Use custom caching of data\n",
    "cache_dir = Path(\"data\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "cache_file = cache_dir / \"dataset.csv.gz\"\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading data from cache: {cache_file}\")\n",
    "    dataset = pd.read_csv(cache_file)\n",
    "else:\n",
    "    # Load dataset via folktables, if necessary download from the internet\n",
    "    dataset = data_source.get_data(states=[\"CA\"], download=True)\n",
    "    # Write file to cache\n",
    "    dataset.to_csv(cache_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef65cc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RT</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>SPORDER</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>REGION</th>\n",
       "      <th>ST</th>\n",
       "      <th>ADJINC</th>\n",
       "      <th>PWGTP</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>...</th>\n",
       "      <th>PWGTP71</th>\n",
       "      <th>PWGTP72</th>\n",
       "      <th>PWGTP73</th>\n",
       "      <th>PWGTP74</th>\n",
       "      <th>PWGTP75</th>\n",
       "      <th>PWGTP76</th>\n",
       "      <th>PWGTP77</th>\n",
       "      <th>PWGTP78</th>\n",
       "      <th>PWGTP79</th>\n",
       "      <th>PWGTP80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000004</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3701</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7306</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000016</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3755</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>109</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>105</td>\n",
       "      <td>232</td>\n",
       "      <td>226</td>\n",
       "      <td>110</td>\n",
       "      <td>114</td>\n",
       "      <td>217</td>\n",
       "      <td>2</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7319</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000027</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6511</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>46</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>81</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>79</td>\n",
       "      <td>47</td>\n",
       "      <td>44</td>\n",
       "      <td>81</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378812</th>\n",
       "      <td>P</td>\n",
       "      <td>2018HU1400891</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1308</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>94</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>77</td>\n",
       "      <td>21</td>\n",
       "      <td>171</td>\n",
       "      <td>86</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378813</th>\n",
       "      <td>P</td>\n",
       "      <td>2018HU1400893</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7108</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>172</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>173</td>\n",
       "      <td>53</td>\n",
       "      <td>288</td>\n",
       "      <td>47</td>\n",
       "      <td>199</td>\n",
       "      <td>181</td>\n",
       "      <td>164</td>\n",
       "      <td>178</td>\n",
       "      <td>176</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378814</th>\n",
       "      <td>P</td>\n",
       "      <td>2018HU1400893</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7108</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>156</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>142</td>\n",
       "      <td>51</td>\n",
       "      <td>259</td>\n",
       "      <td>46</td>\n",
       "      <td>171</td>\n",
       "      <td>150</td>\n",
       "      <td>152</td>\n",
       "      <td>169</td>\n",
       "      <td>165</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378815</th>\n",
       "      <td>P</td>\n",
       "      <td>2018HU1400893</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7108</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>172</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>178</td>\n",
       "      <td>56</td>\n",
       "      <td>297</td>\n",
       "      <td>56</td>\n",
       "      <td>189</td>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378816</th>\n",
       "      <td>P</td>\n",
       "      <td>2018HU1400893</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7108</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1013097</td>\n",
       "      <td>127</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>131</td>\n",
       "      <td>39</td>\n",
       "      <td>200</td>\n",
       "      <td>43</td>\n",
       "      <td>147</td>\n",
       "      <td>128</td>\n",
       "      <td>138</td>\n",
       "      <td>139</td>\n",
       "      <td>133</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378817 rows × 286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       RT       SERIALNO  DIVISION  SPORDER  PUMA  REGION  ST   ADJINC  PWGTP  \\\n",
       "0       P  2018GQ0000004         9        1  3701       4   6  1013097     32   \n",
       "1       P  2018GQ0000013         9        1  7306       4   6  1013097     45   \n",
       "2       P  2018GQ0000016         9        1  3755       4   6  1013097    109   \n",
       "3       P  2018GQ0000020         9        1  7319       4   6  1013097     34   \n",
       "4       P  2018GQ0000027         9        1  6511       4   6  1013097     46   \n",
       "...    ..            ...       ...      ...   ...     ...  ..      ...    ...   \n",
       "378812  P  2018HU1400891         9        2  1308       4   6  1013097     94   \n",
       "378813  P  2018HU1400893         9        1  7108       4   6  1013097    172   \n",
       "378814  P  2018HU1400893         9        2  7108       4   6  1013097    156   \n",
       "378815  P  2018HU1400893         9        3  7108       4   6  1013097    172   \n",
       "378816  P  2018HU1400893         9        4  7108       4   6  1013097    127   \n",
       "\n",
       "        AGEP  ...  PWGTP71  PWGTP72  PWGTP73  PWGTP74  PWGTP75  PWGTP76  \\\n",
       "0         30  ...       34       60       60        7        8       59   \n",
       "1         18  ...        0        0        0       91       46       46   \n",
       "2         69  ...      105      232      226      110      114      217   \n",
       "3         25  ...       67        0       34       34       69        0   \n",
       "4         31  ...       47       81       10       11       79       47   \n",
       "...      ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "378812    61  ...       26       77       21      171       86       31   \n",
       "378813    40  ...      173       53      288       47      199      181   \n",
       "378814    43  ...      142       51      259       46      171      150   \n",
       "378815    23  ...      178       56      297       56      189      167   \n",
       "378816    18  ...      131       39      200       43      147      128   \n",
       "\n",
       "        PWGTP77  PWGTP78  PWGTP79  PWGTP80  \n",
       "0            33        8       58       32  \n",
       "1             0       89       45        0  \n",
       "2             2      111        2      106  \n",
       "3            34       35        0        0  \n",
       "4            44       81       47       10  \n",
       "...         ...      ...      ...      ...  \n",
       "378812       25       82       82      106  \n",
       "378813      164      178      176      154  \n",
       "378814      152      169      165      148  \n",
       "378815      167      177      180      155  \n",
       "378816      138      139      133      116  \n",
       "\n",
       "[378817 rows x 286 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download additional definition data\n",
    "definition_df = data_source.get_definitions(download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0910e",
   "metadata": {},
   "source": [
    "### Perform Pre-Processing for Selected Task\n",
    "\n",
    "- **ACSIncome**: predict whether an individual’s income is above $50,000, after filtering the ACS PUMS data sample to only include individuals above the age of 16, who reported usual working hours of at least 1 hour per week in the past year, and an income of at least $100. The threshold of $50,000 was chosen so that this dataset can serve as a replacement to UCI Adult, but we also offer datasets with other income cutoffs described in Appendix B.\n",
    "- **ACSPublicCoverage**: predict whether an individual is covered by public health insurance, after filtering the ACS PUMS data sample to only include individuals under the age of 65, and those with an income of less than $30,000. This filtering focuses the prediction problem on low-income individuals who are not eligible for Medicare.\n",
    "- **ACSMobility**: predict whether an individual had the same residential address one year ago, after filtering the ACS PUMS data sample to only include individuals between the ages of 18 and 35. This filtering increases the difficulty of the prediction task, as the base rate of staying at the same address is above 90% for the general population.\n",
    "- **ACSEmployment**: predict whether an individual is employed, after filtering the ACS PUMS data sample to only include individuals between the ages of 16 and 90.\n",
    "- **ACSTravelTime**: predict whether an individual has a commute to work that is longer than 20 minutes, after filtering the ACS PUMS data sample to only include individuals who are employed and above the age of 16. The threshold of 20 minutes was chosen as it is the US-wide median travel time to work in the 2018 ACS PUMS data release\n",
    "\n",
    "- The selected story & task has implications for which fairness metric makes the most sense in the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from folktables import generate_categories\n",
    "from folktables import ACSPublicCoverage\n",
    "\n",
    "# Normally you would create the task with the following snippet\n",
    "# features, label, group = ACSEmployment.df_to_numpy(acs_data)\n",
    "# But this severly limits us in regards to how many protected\n",
    "# groups we can examine and further removes feature lables\n",
    "\n",
    "task = deepcopy(ACSPublicCoverage)\n",
    "\n",
    "# Additional features to extract, that are not part of the task\n",
    "extra_feature_cols = [\"PUMA\"]\n",
    "task._features.extend(extra_feature_cols)\n",
    "\n",
    "categories = generate_categories(features=task.features, definition_df=definition_df)\n",
    "features_org, label_org, group_org = task.df_to_pandas(dataset, categories=categories)\n",
    "\n",
    "# Keep a reference to the original state of featuers\n",
    "features = features_org.copy()\n",
    "\n",
    "# Immediately remove the extra features before they could leak into the task\n",
    "features.drop(columns=extra_feature_cols, inplace=True)\n",
    "extra_features = features_org[extra_feature_cols].copy()\n",
    "\n",
    "label = label_org.copy()\n",
    "group = group_org.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the data\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a4746",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "```\n",
    "# Invert the categories dictionary\n",
    "categories_inverted = {\n",
    "    column: {v: k for k, v in mapping.items()}\n",
    "    for column, mapping in categories.items()\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4bdec58",
   "metadata": {},
   "source": [
    "### Exclude Protected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this will alwqys be n >= 1, even if empty!\n",
    "excluded_features = universe[\"exclude_features\"].split(\"-\")\n",
    "excluded_features_dictionary = {\n",
    "    \"race\": \"RAC1P\",\n",
    "    \"sex\": \"SEX\",\n",
    "    \"immigration\": \"NATIVITY\",\n",
    "}\n",
    "\n",
    "# Code nice names to column names\n",
    "excluded_feature_columns = [\n",
    "    excluded_features_dictionary[f] for f in excluded_features if len(f) > 0 and f != \"none\"\n",
    "]\n",
    "\n",
    "if len(excluded_feature_columns) > 0:\n",
    "    print(f\"Dropping features: {excluded_feature_columns}\")\n",
    "    features.drop(excluded_feature_columns, axis=1, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11bb9c9a",
   "metadata": {},
   "source": [
    "### Continuous Variables: Binning / Log-Scaling / Keeping Them As-Is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8517e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from fairness_multiverse.universe import preprocess_continuous\n",
    "\n",
    "transformer_age, bins_age = preprocess_continuous(source_data=features, column_name=\"AGEP\", configuration=universe[\"preprocess_age\"])\n",
    "transformer_income, bins_income = preprocess_continuous(source_data=features, column_name=\"PINCP\", configuration=universe[\"preprocess_income\"])\n",
    "\n",
    "continuous_processor = make_pipeline(\n",
    "    transformer_age,\n",
    "    transformer_income\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6a8beb2",
   "metadata": {},
   "source": [
    "### Categorical Variables: One-Hot or Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7da117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "all_categorical_columns = list(set(categories.keys()).intersection(set(features.columns)))\n",
    "\n",
    "# For which columns is ordinal encoding even an option?\n",
    "categorical_columns_to_transform = [\n",
    "    'SCHL',\n",
    "    # 'MAR',\n",
    "    # 'SEX',\n",
    "    # 'DIS',\n",
    "    # 'ESP',\n",
    "    # 'CIT',\n",
    "    # 'MIG',\n",
    "    'MIL',\n",
    "    # 'ANC',\n",
    "    # 'NATIVITY',\n",
    "    # 'DEAR',\n",
    "    # 'DEYE',\n",
    "    # 'DREM',\n",
    "    # 'ESR',\n",
    "    # 'ST',\n",
    "    # 'FER',\n",
    "    # 'RAC1P'\n",
    "]\n",
    "\n",
    "# Support to-be-binned continuous variables\n",
    "def add_binned_variable_to_categorical_transformation(colname, values):\n",
    "    if values is not None:\n",
    "        categorical_columns_to_transform.append(colname)\n",
    "        categories[colname] = {val: val for val in values}\n",
    "\n",
    "add_binned_variable_to_categorical_transformation(\"AGEP\", bins_age)\n",
    "add_binned_variable_to_categorical_transformation(\"PINCP\", bins_income)\n",
    "\n",
    "def nested_list(all_categories, columns_to_use):\n",
    "    categories = { col: all_categories[col] for col in columns_to_use }\n",
    "    # Create a nested list from the categories dict\n",
    "    categories_list = [[v for k, v in mapping.items()] for column, mapping in categories.items()]\n",
    "    return categories_list\n",
    "\n",
    "if (universe[\"encode_categorical\"] == \"ordinal\"):\n",
    "    categorical_transformer = OrdinalEncoder(\n",
    "        categories = nested_list(categories, categorical_columns_to_transform),\n",
    "    )\n",
    "elif (universe[\"encode_categorical\"] == \"one-hot\"):\n",
    "    categorical_transformer = OneHotEncoder(\n",
    "        categories = nested_list(categories, categorical_columns_to_transform),\n",
    "        sparse_output=False\n",
    "    )\n",
    "else:\n",
    "    raise \"Unsupported universe option for encode_categorical\"\n",
    "\n",
    "# One-Hot Encode all other cateogircal columns\n",
    "other_categorical_columns = list(set(all_categorical_columns) - set(categorical_columns_to_transform))\n",
    "other_transformer = OneHotEncoder(\n",
    "    categories = nested_list(categories, other_categorical_columns),\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "categorical_preprocessor = ColumnTransformer([\n",
    "        (\"encode_categorical\", categorical_transformer, categorical_columns_to_transform),\n",
    "        (\"encode_categorical_rest\", other_transformer, other_categorical_columns),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a533a3",
   "metadata": {},
   "source": [
    "## Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select stratification strategy\n",
    "if universe[\"stratify_split\"] == \"none\":\n",
    "    stratify = None\n",
    "elif universe[\"stratify_split\"] == \"target\":\n",
    "    stratify = label\n",
    "elif universe[\"stratify_split\"] == \"protected-attribute\":\n",
    "    stratify = features_org[\"RAC1P\"]\n",
    "elif universe[\"stratify_split\"] == \"both\":\n",
    "    # Concatinate both columns\n",
    "    stratify = features_org[\"RAC1P\"].astype(str) + \"-\" + label[\"PUBCOV\"].astype(str)\n",
    "\n",
    "stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(\n",
    "    X_train, X_test,\n",
    "    y_train, y_true,\n",
    "    group_train, group_test,\n",
    "    org_train, org_test\n",
    ") = train_test_split(\n",
    "    features,\n",
    "    label,\n",
    "    group,\n",
    "    features_org,\n",
    "    test_size=0.2,\n",
    "    # Note: The analysis originally used two distinct seeds, one for numpy (defaulting to 2023) and one for the train_test_split (defaulting to 0).\n",
    "    # To allow for exact reproducibility of the original results, as well as specification of only a single seed we base this second seed off the first one.\n",
    "    # If you adapt this code for your own analysis feel free to remove this line and replace it e.g. with e.g. a call to numpy.random.randint.\n",
    "    random_state=abs(parsed_seed - 2023),\n",
    "    stratify=stratify\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f0e9037",
   "metadata": {},
   "source": [
    "## Post-Splitting Processing\n",
    "\n",
    "If e.g. only train data is affected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f46a11",
   "metadata": {},
   "source": [
    "### Exclude Certain Subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract configuration\n",
    "exclude_subgroups_config = universe[\"exclude_subgroups\"].split(\"_\")\n",
    "if len(exclude_subgroups_config) == 1:\n",
    "    exclude_subgroups_config = (exclude_subgroups_config[0], None, None)\n",
    "excl_subgroups_method, excl_subgroup_colname, excl_subgroups_value = exclude_subgroups_config\n",
    "\n",
    "if excl_subgroup_colname == \"race\":\n",
    "    excl_subgroup_column = \"RAC1P\"\n",
    "    excl_subgroup_counts = org_train[excl_subgroup_column].value_counts()\n",
    "elif excl_subgroups_method != \"keep-all\":\n",
    "    raise Exception(\"Unsupported configuration for exclude_subgroups:\" + universe[\"exclude_subgroups\"])\n",
    "\n",
    "if excl_subgroups_method == \"keep-all\":\n",
    "    # Don't need to do anything\n",
    "    excl_subgroup_column = None\n",
    "    excl_subgroup_values = []\n",
    "else:\n",
    "    if excl_subgroups_method == \"drop-smallest\":\n",
    "        drop_smallest_n = int(excl_subgroups_value)\n",
    "        excl_subgroup_values = list(excl_subgroup_counts.tail(drop_smallest_n).index)\n",
    "    elif excl_subgroups_method == \"keep-largest\":\n",
    "        keep_largest_n = int(excl_subgroups_value)\n",
    "        excl_subgroup_values = list(excl_subgroup_counts.tail(\n",
    "            len(excl_subgroup_counts) - keep_largest_n\n",
    "        ).index)\n",
    "    elif excl_subgroups_method == \"drop-name\":\n",
    "        excl_subgroup_values = [excl_subgroups_value]\n",
    "    else:\n",
    "        raise Exception(\"Unsupported configuration for exclude_subgroups:\" + universe[\"exclude_subgroups\"])\n",
    "\n",
    "    if excl_subgroup_column is not None:\n",
    "        print(f\"Dropping values: {excl_subgroup_values}\")\n",
    "        keep_rows_mask = ~org_train[excl_subgroup_column].isin(excl_subgroup_values)\n",
    "\n",
    "    n_rows_to_drop = (~keep_rows_mask).sum()\n",
    "    if n_rows_to_drop > 0:\n",
    "        print(f\"Dropping N = {n_rows_to_drop} ({n_rows_to_drop / len(keep_rows_mask):.2%}) rows from {excl_subgroup_colname}\")\n",
    "        X_train = X_train[keep_rows_mask]\n",
    "        y_train = y_train[keep_rows_mask]\n",
    "        group_train = group_train[keep_rows_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd0e0d",
   "metadata": {},
   "source": [
    "## Fitting the Model\n",
    "\n",
    "Select which model to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbcf50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "if (universe[\"model\"] == \"logreg\"):\n",
    "    model = LogisticRegression()\n",
    "elif (universe[\"model\"] == \"rf\"):\n",
    "    model = RandomForestClassifier()\n",
    "elif (universe[\"model\"] == \"svm\"):\n",
    "    model = SVC()\n",
    "elif (universe[\"model\"] == \"gbm\"):\n",
    "    model = GradientBoostingClassifier()\n",
    "elif (universe[\"model\"] == \"elasticnet\"):\n",
    "    model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.5)\n",
    "else:\n",
    "    raise \"Unsupported universe.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.universe import predict_w_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"continuous_processor\", continuous_processor),\n",
    "    (\"categorical_preprocessor\", categorical_preprocessor),\n",
    "    (\"scale\", StandardScaler() if universe[\"scale\"] == \"scale\" else None),\n",
    "    (\"model\", model),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_prob = model.predict_proba(X_test)\n",
    "y_pred_default = predict_w_threshold(y_prob, 0.5)\n",
    "\n",
    "# Naive prediction\n",
    "accuracy_score(y_true = y_true, y_pred = y_pred_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c72f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea335dd0",
   "metadata": {},
   "source": [
    "## (Fairness) Metrics\n",
    "\n",
    "- Using [Fairlearn](https://fairlearn.org/v0.8/quickstart.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1702fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "colname_to_bin = \"RAC1P\"\n",
    "majority_value = features_org[colname_to_bin].mode()[0]\n",
    "\n",
    "org_test[\"majmin\"] = np.where(org_test[colname_to_bin] == majority_value, \"majority\", \"minority\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "178072b8",
   "metadata": {},
   "source": [
    "The greater of two metrics: `true_positive_rate_difference` and\n",
    "`false_positive_rate_difference`. The former is the difference between the\n",
    "largest and smallest of $P[h(X)=1 | A=a, Y=1]$, across all values :math:`a`\n",
    "of the sensitive feature(s). The latter is defined similarly, but for\n",
    "$P[h(X)=1 | A=a, Y=0]$.\n",
    "The equalized odds difference of 0 means that all groups have the same\n",
    "true positive, true negative, false positive, and false negative rates. [src](https://fairlearn.org/main/api_reference/generated/fairlearn.metrics.equalized_odds_difference.html)\n",
    "\n",
    "> This shouldn't differ based on which class we see as \"good\" or \"bad\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e12cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_universe = universe.copy()\n",
    "example_universe[\"cutoff\"] = example_universe[\"cutoff\"][0]\n",
    "example_universe[\"eval_fairness_grouping\"] = example_universe[\"eval_fairness_grouping\"][0]\n",
    "fairness_dict, metric_frame = universe_analysis.compute_metrics(\n",
    "    example_universe,\n",
    "    y_pred_prob=y_prob,\n",
    "    y_test=y_true,\n",
    "    org_test=org_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6da15",
   "metadata": {},
   "source": [
    "### Overall\n",
    "\n",
    "#### Fairness\n",
    "\n",
    "Main fairness target: **Equalized Odds**.\n",
    "Seems to be a better fit than equal opportunity, since we're not only interested in Y = 1.\n",
    "Seems to be a better fit than demographic parity, since we also care about accuracy, not just equal distribution of preds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a335f",
   "metadata": {},
   "source": [
    "Pick column for computation of fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d960396",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "Overall performance measures, most interesting in relation to the measures split by group below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame.overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e50eb0",
   "metadata": {},
   "source": [
    "### By Group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408b097",
   "metadata": {},
   "source": [
    "Graphical comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a graphic\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Show all metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3df12f",
   "metadata": {},
   "source": [
    "### Equality of opportunity violation\n",
    "\n",
    "```python\n",
    "white_tpr = np.mean(y_pred[(y_true == 1) & (group_test == 1)])\n",
    "black_tpr = np.mean(y_pred[(y_true == 1) & (group_test == 2)])\n",
    "\n",
    "white_tpr - black_tpr\n",
    "```\n",
    "\n",
    "## Final Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_universes = universe_analysis.generate_sub_universes()\n",
    "len(sub_universes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc62dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Uses excl_subgroup_column and values from the global scope\n",
    "\n",
    "def filter_sub_universe_data(sub_universe, org_test):\n",
    "  # Generate an all True mask to start with\n",
    "  keep_rows_mask = np.ones(org_test.shape[0], dtype=bool)\n",
    "\n",
    "  # Potentially remove any subgroups from the test set\n",
    "  if (sub_universe[\"eval_exclude_subgroups\"] == \"exclude-in-eval\"):\n",
    "    if excl_subgroup_column is not None:\n",
    "      assert excl_subgroup_values is not None\n",
    "\n",
    "      exclude_subgroup_eval_mask = ~org_test[excl_subgroup_column].isin(excl_subgroup_values)\n",
    "      keep_rows_mask = keep_rows_mask & exclude_subgroup_eval_mask\n",
    "\n",
    "      n_rows_to_drop = (~exclude_subgroup_eval_mask).sum()\n",
    "      print(f\"[drop subgroups] Dropping N = {n_rows_to_drop} ({n_rows_to_drop / len(keep_rows_mask):.2%}) rows from {excl_subgroup_colname}\")\n",
    "  elif (sub_universe[\"eval_exclude_subgroups\"] == \"keep-in-eval\"):\n",
    "    pass\n",
    "  else:\n",
    "    raise \"Unsupported eval_exclude_subgroups\"\n",
    "\n",
    "  # Potentially use a smaller and more \"convenient\" subset of the data to do evalaution on\n",
    "  if (sub_universe[\"eval_on_subset\"] == \"full\"):\n",
    "    pass\n",
    "  else:\n",
    "    if sub_universe[\"eval_on_subset\"].startswith(\"locality\"):\n",
    "      # Filter based on locality / region\n",
    "      # Step 1: Decide which regions to keep\n",
    "      if (sub_universe[\"eval_on_subset\"] == \"locality-largest-only\"):\n",
    "        # Use the largest PUMA region\n",
    "        puma_regions_to_keep = [org_test[\"PUMA\"].value_counts().idxmax()]\n",
    "      elif (sub_universe[\"eval_on_subset\"] == \"locality-whitest-only\"):\n",
    "        # Find the majority class on the prot. attribute\n",
    "        majority_class = org_test[\"RAC1P\"].value_counts().index[0]\n",
    "        majority_class\n",
    "\n",
    "        # Find the PUMA region with the highest share of the majority class\n",
    "        counts = pd.DataFrame()\n",
    "        counts[\"full\"] = org_test[\"PUMA\"].value_counts(sort=False)\n",
    "        counts[\"majority\"] = org_test[org_test[\"RAC1P\"] == majority_class][\"PUMA\"].value_counts(sort=False)\n",
    "        counts[\"fraction\"] = counts[\"majority\"] / counts[\"full\"]\n",
    "\n",
    "        # Use the PUMA region with the highest share of the majority class\n",
    "        majority_puma_id = counts.sort_values(by=\"fraction\", ascending=False).index[0]\n",
    "        puma_regions_to_keep = [majority_puma_id]\n",
    "      elif (sub_universe[\"eval_on_subset\"] == \"locality-city-la\"):\n",
    "        puma_regions_to_keep = list(range(3701, 3769+1))\n",
    "      elif (sub_universe[\"eval_on_subset\"] == \"locality-city-sf\"):\n",
    "        puma_regions_to_keep = list(range(7501, 7507+1))\n",
    "\n",
    "      # Step 2: Keep only those regions\n",
    "      print(f\"Keeping the following PUMA regions: {puma_regions_to_keep}\")\n",
    "      eval_on_subset_mask = org_test[\"PUMA\"].isin(puma_regions_to_keep)\n",
    "    elif (sub_universe[\"eval_on_subset\"] == \"exclude-military\"):\n",
    "      # Only keep non-military personnel\n",
    "      eval_on_subset_mask = (org_test[\"MIL\"].isin([\"Never served in the military\", \"N/A (less than 17 years old)\"]))\n",
    "    elif (sub_universe[\"eval_on_subset\"] == \"exclude-non-citizens\"):\n",
    "      # Only keep US citizens\n",
    "      eval_on_subset_mask = ~(org_test[\"CIT\"] == \"Not a citizen of the U.S.\")\n",
    "    else:\n",
    "      raise \"Unsupported eval_on_subset\"\n",
    "\n",
    "    keep_rows_mask = keep_rows_mask & eval_on_subset_mask\n",
    "\n",
    "    n_rows_to_drop = (~eval_on_subset_mask).sum()\n",
    "    print(f\"[subset] Dropping N = {n_rows_to_drop} ({n_rows_to_drop / len(keep_rows_mask):.2%}) rows\")\n",
    "\n",
    "  n_rows_to_drop = (~keep_rows_mask).sum()\n",
    "  print(f\"[TOTAL] Dropping N = {n_rows_to_drop} ({n_rows_to_drop / len(keep_rows_mask):.2%}) rows. Final size: {keep_rows_mask.sum()}.\")\n",
    "\n",
    "  return keep_rows_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6976413",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = universe_analysis.generate_final_output(\n",
    "    y_pred_prob=y_prob,\n",
    "    y_test=y_true,\n",
    "    org_test=org_test,\n",
    "    save=True,\n",
    "    filter_data=filter_sub_universe_data\n",
    ")\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb937e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness-multiverse-jpsnutmQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "11d6b0fec11ff4c9339ef5e7bc4c34b716123d4de08ca335068ad050c77a570c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
